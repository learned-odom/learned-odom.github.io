<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Learned Odometry.">
  <meta name="keywords" content="Odometry Estimation, State Estimation, Quadruped Robots, Sim-To-Real">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Legolas: Deep Leg-Inertial Odometry</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-ZYH3N96LN5');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/slick.css">
  <link rel="stylesheet" href="./static/css/slick-theme.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/slick.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Legolas: Deep Leg-Inertial Odometry</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup></sup>Anonymous Authors</span>
            <br>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Estimating odometry, where an accumulating position and rotation is tracked,
                has critical applications in many areas of robotics as a form of state estimation
                such as in SLAM, navigation, and controls. During deployment of a legged robot,
                a vision system's tracking can easily get lost. Instead, using only the onboard leg
                and inertial sensor for odometry is a promising alternative. Previous methods in
                estimating leg-inertial odometry require analytical modeling or collecting high-quality
                real-world trajectories to train a model. Analytical modeling is specific to each robot,
                requires manual fine-tuning, and doesn't always capture real-world phenomena such as
                slippage. Previous work learning legged odometry still relies on collecting real-world
                data, this has been shown to not perform well out of distribution. In this work, we
                show that it is possible to estimate the odometry of a legged robot without any
                analytical modeling or real-world data collection. In this paper, we present Legolas,
                the first method that accurately estimates odometry in a purely data-driven fashion
                for quadruped robots. We deploy our method on two real-world quadruped robots in both
                indoor and outdoor environments. In the indoor scenes, our proposed method accomplishes
                a relative pose error that is 73% less than an analytical filtering-based approach
                and 87.5% less than a real-world behavioral cloning approach.               
            </p>
          </div>
        </div>
      </div>
            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                  <h2 class="title is-3">Supplementary Video</h2>
                  <div class="publication-video">
                      <iframe src="./legolas/video/main.mp4" frameborder="0"
                          allow="autoplay; encrypted-media" allowfullscreen></iframe>
                  </div>
              </div>
          </div>
      </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop is-centered">
        <center>
            <h2 class="title is-3">Legolas</h2>
        </center>
        <br>
        <div class="content has-text-justified">
            <p>
                <center><u><b>Leg</b></u>ged <u><b>o</b></u>dometry <u><b>l</b></u>earned <u><b>a</b></u>s <u><b>s</b></u>imulated, is the first complete data-driven method for accurate and real-time state estimation. </center>
            </p>
            <center>
                <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                    <source src="./legolas/video/overview_clipped.mp4" type="video/mp4">
                </video>
            </center><br>
            <center>Legolas is effective as it scales with simulation data and doesn't require analytical modeling of the robot. This flexibility in dataset curation and success at scale allows for training and deployment across multiple robots. In the video below, red is the odometry prediction from Legolas, and blue is the ground truth pose over time.</center>
            <center>
              <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                  <source src="./legolas/video/rollouts_sim_crop.mp4" type="video/mp4">
              </video>
          </center>
        </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop is-centered">
        <center>
            <h2 class="title is-3">Comparisons to Baselines</h2>
        </center>
        <br>
        <div class="content has-text-justified">
            <p>
                In this baseline rollout, Legolas is compared to relevent filtering-based and learning-based baselines. In this rollout, the behavorial cloning baseline that was trained from real-world data fails to even 
                leave the area where the robot started. VINS-Fusion, as a visual-baseline in this rollout failed due to lighting reflections causing the 
                estimate to diverge. The filtering baseline, built off analytical models closely tracks the ground-truth trajectory. However, it falls
                in quality when compared to Legolas. (Third person view and map are for visualization purposes.)
            </p>
            <!-- <div class="videos-flex"> -->
            <center>
                <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                    <source src="./legolas/video/baselines.mp4" type="video/mp4">
                </video>
            </center>
            <!-- </div> -->
        </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop is-centered">
        <center>
            <h2 class="title is-3">Stairs in Indoors Scene</h2>
        </center>
        <br>
        <div class="content has-text-justified">
            <p>
                We are able to deploy legolas across diverse terrains such as onto stairs, where it can estimate accurate forward (x), upward (z), and pitch state.
            </p>
            <!-- <div class="videos-flex"> -->
            <center>
                <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                    <source src="./legolas/video/stairs_full.mp4" type="video/mp4">
                </video>
            </center>
            <!-- </div> -->
        </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop is-centered">
        <center>
            <h2 class="title is-3">In the Wild</h2>
        </center>
        <br>
        <div class="content has-text-justified">
            <p>
                Deploying Legolas, onto a rollout of 180m, it is able to accurately predict the pose of the robot with very little drift outdoors.
            </p>
            <!-- <div class="videos-flex"> -->
            <center>
                <video poster="" id="go1-inside-2-main" autoplay controls muted loop playsinline width="80%">
                    <source src="./legolas/video/outdoors.mp4" type="video/mp4">
                </video>
            </center>
            <!-- </div> -->
        </div>
    </div>
  </section>



    <br>
    <center class="is-size-10">
      The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
          class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
